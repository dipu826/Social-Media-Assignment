{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a646f2-ca06-47eb-95c4-ec29145091a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading the client information\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport redditClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7149aa7e-b193-401b-aa1a-96b34c433dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/priyankatiwari/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyankatiwari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages\n",
    "from redditClient import redditClient\n",
    "import matplotlib.pyplot as plt\n",
    "import praw\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import string\n",
    "import nltk\n",
    "from langdetect import detect, DetectorFactory\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from colorama import Fore, Back, Style\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71237041-fe88-4df0-b46c-a3a3c120bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit name we interested in getting the hot submissions, in this case it's 'vegan'\n",
    "sSubredditName = 'Elon Musk'\n",
    "# maximum number of hot submissions(posts)\n",
    "hotLimit = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "514123c6-7d6c-48fa-a805-2109f88fbb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'data.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import praw  # Make sure you have the praw library installed\n",
    "\n",
    "# Get Reddit client from reddit_client.py\n",
    "reddit = redditClient()\n",
    "\n",
    "\n",
    "# List of subreddits to search\n",
    "subreddits = ['all', 'ElonMusk', 'SpaceX', 'technology', 'twitter','tesla']  # Add more subreddits as needed\n",
    "\n",
    "# Initialize a list to hold all posts data\n",
    "all_posts_data = []\n",
    "\n",
    "# Loop through each subreddit\n",
    "for subreddit_name in subreddits:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    elon_posts = subreddit.search('Elon Musk', limit=400)  # Adjust the limit as needed\n",
    "\n",
    "    # Extract relevant data for each subreddit\n",
    "    post_data = []\n",
    "    for post in elon_posts:\n",
    "        if post.created_utc >= one_month_ago_timestamp:\n",
    "            comments_data = []\n",
    "            for comment in post.comments.list():\n",
    "                if isinstance(comment, praw.models.Comment):\n",
    "                    comments_data.append({\n",
    "                        'Comment': comment.body,\n",
    "                        'Comment_Author': comment.author.name if comment.author else 'Unknown',\n",
    "                        'Comment_Date': int(comment.created_utc)  # Add comment date as Unix timestamp\n",
    "                    })\n",
    "\n",
    "            post_data.append({\n",
    "                'Title': post.title,\n",
    "                'Body': post.selftext if post.selftext else 'No text available',\n",
    "                'author': post.author.name if post.author else 'Unknown',\n",
    "                'score': post.score,\n",
    "                'Comments': comments_data,\n",
    "                'created': int(post.created_utc)  # Add post date as Unix timestamp\n",
    "            })\n",
    "    \n",
    "    # Append the data for the current subreddit to all_posts_data\n",
    "    all_posts_data.extend(post_data)\n",
    "\n",
    "# Wrap the all_posts_data list inside a dictionary with \"submission\" keys\n",
    "output_data = {\n",
    "    \"submission\": all_posts_data\n",
    "}\n",
    "\n",
    "# Save the extracted data as a JSON file\n",
    "with open('data.json', 'w') as json_file:\n",
    "    json.dump(output_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Data saved to 'data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a62fe1-79b8-4bef-8f18-5603a4468c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585644c-3a50-4318-a4d5-2d21ba853e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
